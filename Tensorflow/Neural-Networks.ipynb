{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08bb34c9-a7a3-4aad-bf64-bfccc82f16e8",
   "metadata": {},
   "source": [
    "### The linear algebra of dense layers\n",
    "- The input layer contains 3 features -- education, marital status, and age -- which are available as borrower_features. The hidden layer contains 2 nodes and the output layer contains a single node.\n",
    "- For each layer, you will take the previous layer as an input, initialize a set of weights, compute the product of the inputs and weights, and then apply an activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6cb2fe-a081-4097-ae7b-c818239538fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize bias1\n",
    "bias1 = Variable(1.0)\n",
    "\n",
    "# Initialize weights1 as 3x2 variable of ones\n",
    "weights1 = Variable(ones((3,2)))\n",
    "\n",
    "# Perform matrix multiplication of borrower_features and weights1\n",
    "product1 = matmul(borrower_features,weights1)\n",
    "\n",
    "# Apply sigmoid activation function to product1 + bias1\n",
    "dense1 = keras.activations.sigmoid(product1 + bias1)\n",
    "\n",
    "# Print shape of dense1\n",
    "print(\"\\n dense1's output shape: {}\".format(dense1.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47bd5c5-a511-407b-b0f8-856cedad7667",
   "metadata": {},
   "source": [
    "- Initialize weights2 as a variable using a 2x1 tensor of ones.\n",
    "- Compute the product of dense1 by weights2 using matrix multiplication.\n",
    "- Use a sigmoid activation function to transform product2 + bias2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6d44bd-71d7-4b16-aa3f-bf7cfd3c9329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From previous step\n",
    "bias1 = Variable(1.0)\n",
    "weights1 = Variable(ones((3, 2)))\n",
    "product1 = matmul(borrower_features, weights1)\n",
    "dense1 = keras.activations.sigmoid(product1 + bias1)\n",
    "\n",
    "# Initialize bias2 and weights2\n",
    "bias2 = Variable(1.0)\n",
    "weights2 = Variable(ones((2, 1)))\n",
    "\n",
    "# Perform matrix multiplication of dense1 and weights2\n",
    "product2 = matmul(dense1,weights2)\n",
    "\n",
    "# Apply activation to product2 + bias2 and print the prediction\n",
    "prediction = keras.activations.sigmoid(product2 + bias2)\n",
    "print('\\n prediction: {}'.format(prediction.numpy()[0,0]))\n",
    "print('\\n actual: 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a84acbd-aa21-4fca-a37e-63afdc7f4c7b",
   "metadata": {},
   "source": [
    "### Using the dense layer operation\n",
    "- In this example, we'll skip the linear algebra and let keras work out the details. This will allow us to construct the network below, which has 2 hidden layers and 10 features, using less code than we needed for the network with 1 hidden layer and 3 features.\n",
    "- To construct this network, we'll need to define three dense layers, each of which takes the previous layer as an input, multiplies it by weights, and applies an activation function. Note that input data has been defined and is available as a 100x10 tensor: borrower_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adaf6c2-bf6f-4ddb-a3ed-fb310aa9b0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the first dense layer\n",
    "dense1 = keras.layers.Dense(7, activation='sigmoid')(borrower_features)\n",
    "\n",
    "# Define a dense layer with 3 output nodes\n",
    "dense2 = keras.layers.Dense(3,activation='sigmoid')(dense1)\n",
    "\n",
    "# Define a dense layer with 1 output node\n",
    "predictions = keras.layers.Dense(1,activation='sigmoid')(dense2)\n",
    "\n",
    "# Print the shapes of dense1, dense2, and predictions\n",
    "print('\\n shape of dense1: ', dense1.shape)\n",
    "print('\\n shape of dense2: ', dense2.shape)\n",
    "print('\\n shape of predictions: ', predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a75f689-a2e6-4727-bb7b-6275364017b7",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "- These are mathematical function applied to the output of a neuron. The purpose fo an activation function is to introduce non-linearity to the model, allowing the network to learn and represent complex patterns in the data.\n",
    "- A typical hidden layer consists of two operations:\n",
    "    - Linear - performs matrix multiplication\n",
    "    - Non-linear - applies an activation function\n",
    "- Example of Activation function include:\n",
    "    - The sigmoid activation - applied on the output layer of binary classification\n",
    "    - The relu activation - applied in all the hidden layers\n",
    "    - Teh softmax activation - applied in the output layer in classification with more than two classes     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb0477b-d6c9-43d9-b27d-edd1a8256e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input layer\n",
    "inputs = tf.constant(borrower_features,tf.float32)\n",
    "\n",
    "# Define dense layer 1\n",
    "dense1 = tf.keras.layers.Dense(16,activation='relu')(inputs)\n",
    "\n",
    "# Define dense layer 2\n",
    "dense2 = tf.keras.layers.Dense(8,activation='sigmoid')(dense1)\n",
    "\n",
    "# Define output layer\n",
    "output1 = tf.keras.layers.Dense(4,activation='softmax')(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69255532-f2fa-4da5-9686-88c2980de3df",
   "metadata": {},
   "source": [
    "### Binary classification problems\n",
    "- In this exercise, you will again make use of credit card data. The target variable, default, indicates whether a credit card holder defaults on his or her payment in the following period. Since there are only two options--default or not--this is a binary classification problem.\n",
    "- Define inputs as a 32-bit floating point constant tensor using bill_amounts.\n",
    "- Set dense1 to be a dense layer with 3 output nodes and a relu activation function.\n",
    "- Set dense2 to be a dense layer with 2 output nodes and a relu activation function.\n",
    "- Set the output layer to be a dense layer with a single output node and a sigmoid activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b79f19-b199-4ad0-b470-8f9ec3f7d470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct input layer from features\n",
    "inputs = constant(bill_amounts,float32)\n",
    "\n",
    "# Define first dense layer\n",
    "dense1 = keras.layers.Dense(3, activation='relu')(inputs)\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = keras.layers.Dense(2,activation='relu')(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = keras.layers.Dense(1,activation='sigmoid')(dense2)\n",
    "\n",
    "# Print error for first five examples\n",
    "error = default[:5] - outputs.numpy()[:5]\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba86f54-95f9-42b1-a330-f0992b37a5c1",
   "metadata": {},
   "source": [
    "### Multiclass classification problems\n",
    "- In this exercise, we expand beyond binary classification to cover multiclass problems. A multiclass problem has targets that can take on three or more values. In the credit card dataset, the education variable can take on 6 different values, each corresponding to a different level of education.\n",
    "- Define the input layer as a 32-bit constant tensor using borrower_features.\n",
    "- Set the first dense layer to have 10 output nodes and a sigmoid activation function.\n",
    "- Set the second dense layer to have 8 output nodes and a rectified linear unit activation function.\n",
    "- Set the output layer to have 6 output nodes and the appropriate activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57443c4-aebd-489e-aa9e-d2bdfeab3e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct input layer from borrower features\n",
    "inputs = constant(borrower_features,float32)\n",
    "\n",
    "# Define first dense layer\n",
    "dense1 = keras.layers.Dense(10, activation='sigmoid')(inputs)\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = keras.layers.Dense(8, activation='relu')(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = keras.layers.Dense(6, activation='softmax')(dense2)\n",
    "\n",
    "# Print first five predictions\n",
    "print(outputs.numpy()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2290d164-18d8-4e01-8013-e380b60bea5f",
   "metadata": {},
   "source": [
    "### The dangers of local minima\n",
    "- Consider the plot of the following loss function, loss_function(), which contains a global minimum, marked by the dot on the right, and several local minima, including the one marked by the dot on the left.\n",
    "- In this exercise, you will try to find the global minimum of loss_function() using keras.optimizers.SGD(). You will do this twice, each time with a different initial value of the input to loss_function(). First, you will use x_1, which is a variable with an initial value of 6.0. Second, you will use x_2, which is a variable with an initial value of 0.3. Note that loss_function() has been defined and is available\n",
    "   - Set opt to use the stochastic gradient descent optimizer (SGD) with a learning rate of 0.01.\n",
    "   - Perform minimization using the loss function, loss_function(), and the variable with an initial value of 6.0, x_1.\n",
    "   - Perform minimization using the loss function, loss_function(), and the variable with an initial value of 0.3, x_2.\n",
    "   - Print x_1 and x_2 as numpy arrays and check whether the values differ. These are the minima that the algorithm identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6660d1f2-8396-4168-9022-cff31c6c6ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize x_1 and x_2\n",
    "x_1 = Variable(6.0,float32)\n",
    "x_2 = Variable(0.3,float32)\n",
    "\n",
    "# Define the optimization operation\n",
    "opt = keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "for j in range(100):\n",
    "\t# Perform minimization using the loss function and x_1\n",
    "\topt.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
    "\t# Perform minimization using the loss function and x_2\n",
    "\topt.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
    "\n",
    "# Print x_1 and x_2 as numpy arrays\n",
    "print(x_1.numpy(), x_2.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf4df2c-3b87-4542-a9b5-bace1b0bb8a1",
   "metadata": {},
   "source": [
    "### Avoiding local minima\n",
    "- The problem above showed how easy it is to get stuck in local minima. We had a simple optimization problem in one variable and gradient descent still failed to deliver the global minimum when we had to travel through local minima first. One way to avoid this problem is to use momentum, which allows the optimizer to break through local minima. We will again use the loss function from the previous problem, which has been defined and is available for you as loss_function().\n",
    "- Several optimizers in tensorflow have a momentum parameter, including SGD and RMSprop. You will make use of RMSprop in this exercise\n",
    "    - Set the opt_1 operation to use a learning rate of 0.01 and a momentum of 0.99.\n",
    "    - Set opt_2 to use the root mean square propagation (RMS) optimizer with a learning rate of 0.01 and a momentum of 0.00.\n",
    "    - Define the minimization operation for opt_2.\n",
    "    - Print x_1 and x_2 as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db57a80c-33c8-41dc-87f3-126a71e95d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize x_1 and x_2\n",
    "x_1 = Variable(0.05,float32)\n",
    "x_2 = Variable(0.05,float32)\n",
    "\n",
    "# Define the optimization operation for opt_1 and opt_2\n",
    "opt_1 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.99)\n",
    "opt_2 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.00)\n",
    "\n",
    "for j in range(100):\n",
    "\topt_1.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
    "    # Define the minimization operation for opt_2\n",
    "\topt_2.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
    "\n",
    "# Print x_1 and x_2 as numpy arrays\n",
    "print(x_1.numpy(), x_2.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f8cd4e-f763-4ae7-977e-7a4f3e26390e",
   "metadata": {},
   "source": [
    "### Initialization in Tensorflow\n",
    "- A good initialization can reduce the amount of time needed to find the global minimum. In this exercise, we will initialize weights and biases for a neural network that will be used to predict credit card default decisions. To build intuition, we will use the low-level, linear algebraic approach, rather than making use of convenience functions and high-level keras operations. We will also expand the set of input features from 3 to 23.\n",
    "     - Initialize the layer 1 weights, w1, as a Variable() with shape [23, 7], drawn from a normal distribution.\n",
    "     - Initialize the layer 1 bias using ones.\n",
    "     - Use a draw from the normal distribution to initialize w2 as a Variable() with shape [7, 1].\n",
    "     - Define b2 as a Variable() and set its initial value to 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a4df92-f039-43ac-a8ba-0410bb2ecf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the layer 1 weights\n",
    "w1 = Variable(random.normal([23, 7]))\n",
    "\n",
    "# Initialize the layer 1 bias\n",
    "b1 = Variable(ones([7]))\n",
    "\n",
    "# Define the layer 2 weights\n",
    "w2 = Variable(random.normal([7,1]))\n",
    "\n",
    "# Define the layer 2 bias\n",
    "b2 = Variable(ones([0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac40cd51-e279-465c-82df-8bd23a14be74",
   "metadata": {},
   "source": [
    "### Defining the model and loss function\n",
    "- In this exercise, you will train a neural network to predict whether a credit card holder will default. The features and targets you will use to train your network are available in the Python shell as borrower_features and default. You defined the weights and biases in the above.\n",
    "- Note that the predictions layer is defined as , where  is the sigmoid activation, layer1 is a tensor of nodes for the first hidden dense layer, w2 is a tensor of weights, and b2 is the bias tensor.\n",
    "- The trainable variables are w1, b1, w2, and b2. Additionally, the following operations have been imported for you: keras.activations.relu() and keras.layers.Dropout().\n",
    "    - Apply a rectified linear unit activation function to the first layer.\n",
    "    - Apply 25% dropout to layer1.\n",
    "    - Pass the target, targets, and the predicted values, predictions, to the cross entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5553f089-0407-4cb0-9baf-cd7fcc374920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def model(w1, b1, w2, b2, features = borrower_features):\n",
    "\t# Apply relu activation functions to layer 1\n",
    "\tlayer1 = keras.activations.relu(matmul(features, w1) + b1)\n",
    "    # Apply dropout rate of 0.25\n",
    "\tdropout = keras.layers.Dropout(0.25)(layer1)\n",
    "\treturn keras.activations.sigmoid(matmul(dropout, w2) + b2)\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(w1, b1, w2, b2, features = borrower_features, targets = default):\n",
    "\tpredictions = model(w1, b1, w2, b2)\n",
    "\t# Pass targets and predictions to the cross entropy loss\n",
    "\treturn keras.losses.binary_crossentropy(targets, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f5bf68-f752-4351-a37d-1e22a4dc5d9c",
   "metadata": {},
   "source": [
    "### Training neural networks with TensorFlow\n",
    "- In the sample above, you defined a model, model(w1, b1, w2, b2, features), and a loss function, loss_function(w1, b1, w2, b2, features, targets), both of which are available to you in this exercise. You will now train the model and then evaluate its performance by predicting default outcomes in a test set, which consists of test_features and test_targets and is available to you. The trainable variables are w1, b1, w2, and b2.\n",
    "  - Set the optimizer to perform minimization.\n",
    "  - Add the four trainable variables to var_list in the order in which they appear as arguments to loss_function().\n",
    "  - Use the model and test_features to predict the values for test_targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10dcc41-5b40-4e91-93bb-c72c2fd70049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "for j in range(100):\n",
    "    # Complete the optimizer\n",
    "\topt.minimize(lambda: loss_function(w1, b1, w2, b2), \n",
    "                 var_list=[w1, b1, w2, b2])\n",
    "\n",
    "# Make predictions with model using test features\n",
    "model_predictions = model(w1, b1, w2, b2, test_features)\n",
    "\n",
    "# Construct the confusion matrix\n",
    "confusion_matrix(test_targets, model_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
